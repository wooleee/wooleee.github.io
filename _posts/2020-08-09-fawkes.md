---
layout: posts
title: "[CV][GAN][EN]Fawkes: Protecting Privacy against Unauthorized Deep Learning Models"
excerpt: "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models"
published: False
categories:
- Paper
tags:
- Study
- ComputerVision
last_modified_at: 2020-08-09
comments: true
author: "WooLee"
---

> * I sometimes call cycleGAN "model" in this post.

# 0. Found at

* [Paper](https://arxiv.org/abs/2002.08327v2)

* Proliferation of powerful facial recognition systems poses a real threat to personal privacy. 
* We need tools to protect ourselves from potential misuses of unauthorized facial recognition systems. 
* In this paper, we propose Fawkes, a system that helps individuals inoculate their images against unauthorized facial recognition models. 
* Fawkes achieves this by helping users add imperceptible pixel-level changes, called "cloaks", to their own photos before releasing them. 
* When used to train facial recognition models, these "cloaked" images produce functional models that consistently cause normal images of the user to be misidentified. 
* We experimentally demonstrate that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. 
* Finally, we show that Fawkes is robust against a variety of countermeasures that try to detect or disrupt image cloaks.

# 1. Introduction  

* Today’s proliferation of powerful facial recognition models poses a real threat to personal privacy. 

* Opportunities for misuse of this technology are numerous and potentially disastrous. Anywhere we go, we can be identified at any time through street cameras, video doorbells, security cameras, and personal cellphones. Stalkers can find out our identity and social media profiles with a single snapshot

* It raises the need tools to protect themselves from being identified by unauthorized facial recognition models. 

* Fawkes, a system that helps individ- uals to inoculate their images against unauthorized facial recognition models at any time without significantly dis- torting their own photos, achieves this by helping users adding imperceptible pixel-level changes (“cloaks”) to their own photos

* If collected by a third-party “tracker” and used to train a facial recog- nition model to recognize the user, these “cloaked” images would produce functional models that consistently misidentify them. 

<img src = "/assets/img/fawkes/fig1.png"><br>

* Distortion or “cloaking” algorithm **takes the user’s photos and computes minimal perturbations that shift them significantly in the feature space** of a facial recognition model

# 2. Background and Related Work

## 2.1. Protecting Privacy via Evasion Attacks
#### Methods
* Relies on creating adversarial examples, inputs to the model designed to cause misclassification. Creating specially printed glasses that cause the wearer to be misidentified is an example

* Or “adversarial patches” that target “person identification” models, making it difficult for models to recognize the wearer as a person in an image 

#### Limitations
1. Methods require the user to wear fairly obvious and conspicuous accessories (hats, glasses, sweaters) so it is **impractical for normal use.** 

2. And, in order to evade tracking, methods require full and unrestricted access (white box access) to the precise model tracking them. &#8594;	They are easily broken (and user privacy compromised) by any tracker that **updates its model.**



## 2.2. Protecting Privacy via Poisoning Attacks
An alternative to evading models is to disrupt their training.

### 2.2.1. Clean Label Attacks<br>
#### Methods
* Poisoning attack injects “correctly” labeled poison images into training data, **causing a model trained on this data to misclassify** a specific image of interest

#### Limitations
1. It only causes misclassification on a single, preselected image, whereas user privacy protection requires the misclassification of any current or future image of the protected user (i.e. an entire model class)

2. Clean label attacks do not transfer well to dif- ferent models, especially models trained from scratch. Even between models trained on the same data, the attack only transfers with 30% success rate

3. Easily detectable through anomaly detection in the feature space

> Model Corruption Attacks<br>

### 2.2.2. Model Corruption Attacks.<br>
#### Methods
* Modifing images such that they degrade the accuracy of a model trained on them &#8594; to spread these poisoned images in order to **discourage unauthorized data collection and model training.** 


## 2.3. Other Related Work

### 2.3.1. Privacy-Preserving Machine Learning
* Recent work has shown that ML models can memorize (and **subsequently leak**) parts of their training data &#8594; Vulnerable to be exploited to expose private details about members of the training dataset.

* These attacks have spurred a push towards differentially **private model training**( protecting sensitive characteristics of training data)
* But, these techniques imply a **trusted model trainer** and are **ineffective against an unauthorized model trainer.**

### 2.3.2. Feature Extractors & Transfer Learning
* **Transfer learning** uses existing pretrained models as a basis for quickly training models for customized classification tasks, using less training data. 

* It is commonly used to deploy complex ML models (e.g. **facial recognition or image segmentation**) at **reasonable training costs**.

* Settings
    * pre-trained feature extractor $\Phi$ is passed on to a new model $F_{\theta}$
    * then, model $F_{\theta}$ can be created by appending a few additional layers to $\Phi$ and only training those layers.

* Therefore transfer learning is most effective when the **feature extractor and model are trained on similar datasets**.

### 2.3.3. Others
* the concept of protecting individual privacy against invasive technologies extends beyond the image domain. 
* For example, wearable devices that restore personal agency using digital jammers to prevent audio eavesdropping by ubiquitous digital home assistants.


# 3. Protecting Privacy via Cloaking
* To help protect the privacy of a **user against** unauthorized facial recognition models trained by a third-party **tracker** on the user’s images, Fawkes can be applied.

* Fawkes achieve this by adding subtle perturbations(which is called **"cloaks"**) to the users' images before sharing them.

## 3.1. Assumptions and Threat Model
* For User
<img src = "/assets/img/fawkes/fig2.png"><br>

  * Goal: To share their photos w/o unknowingly helping 3rd party trackers build facial recognition models(that is recognizing them)
  * Design of Cloak
    * cloaks should be imperceptible and **not impact normal use of the image**
    * classifying normal, uncloaked images, models trained on cloaked images should recognize the underlying person with low accuracy

* Tracker/Model Trainer
  * Goal
    * To build a powerful model to track many users rather than targeting a single specific person.
  * Assumption
    * the tracker is a third party without direct access to user’s personal photos 
    * tracker could be a company like Clearview.ai, a government entity, or even an individual. The tracker has significant computational resources.

* Real World Limitations
  * Privacy benefits of Fawkes **rely on users applying our cloaking technique** to the majority of images of their likeness **before posting** online

## 3.2 Overview and Intuition

* DNN models are trained to identify and extract (often hidden) features in input data and use them to perform classification. 
* And Yet their ability to identify features is easily disrupted by data poisoning attacks during model training, where **small perturbations on training data** with a particular label (l) can shift the model’s view of what features uniquely identify l.

## 3.3 Computing Cloak Perturbations

goal of making
the features learned from cloaked photos highly dissimilar from those learned from original (uncloaked) photos.

* Notations

$x$ : A's Uncloaked Image
$x_T$ : Target Image(other than A's Image) used to generate cloak for $x$
$\delta(x,x_T)$ : cloak computed for A's image $x$ based on an image $x_T$ from label $T$.
$x \bigoplus \delta(x,x_T)$ : cloaked version of A's image $x$
$\Phi$ : Feature extractor used by facial recognition model
$\Phi(x)$ : Feature vector(or feature representation) extracted from input $x$

* Cloaking to Maximize Feature Deviation  
$\tag{1} max_{\delta}Dist(\Phi(x), \Phi(x \bigoplus \delta(x,x_T)))$
subject to $\vert \delta(x,x_T) \vert < \rho$

> Note<br>  
 
$Dist(.)$ - computes the distance of 2 feature vectors
$\vert \delta \vert$ - measures the perceptual perturbation caused by cloaking
$\rho$ - is the perceptual perturbation budget.
$x_T$ - serves as a landmark, enabling fast and efficient search for the input pertur- bation that leads to large changes in feature representation. Therefore and foremost, T should be very **dissimilar from A's image** in the feature space. 

* Image-specific Cloaking


<br><br>**The End.**